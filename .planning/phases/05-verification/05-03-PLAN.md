---
phase: 05-verification
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - skills/inspect/SKILL.md
autonomous: true

must_haves:
  truths:
    - "Running /director:inspect with no arguments checks the current step"
    - "Running /director:inspect goal checks the current goal"
    - "Running /director:inspect all checks the entire project"
    - "Inspect always shows results (even 'everything checks out') since the user explicitly asked"
    - "Inspect runs both Tier 1 structural and Tier 2 behavioral verification"
    - "Issues are presented in two-severity format with auto-fix offer"
    - "Passing verification triggers celebration with progress feedback"
  artifacts:
    - path: "skills/inspect/SKILL.md"
      provides: "Complete on-demand verification command"
      contains: "director-verifier"
    - path: "skills/inspect/SKILL.md"
      provides: "Scope resolution from arguments"
      contains: "scope"
    - path: "skills/inspect/SKILL.md"
      provides: "Tier 2 behavioral checklist generation"
      contains: "checklist"
  key_links:
    - from: "skills/inspect/SKILL.md"
      to: "agents/director-verifier.md"
      via: "Spawns verifier with scope-appropriate context"
      pattern: "director-verifier"
    - from: "skills/inspect/SKILL.md"
      to: "agents/director-debugger.md"
      via: "Spawns debugger for auto-fix when user approves"
      pattern: "director-debugger"
    - from: "skills/inspect/SKILL.md"
      to: ".director/STATE.md"
      via: "Reads state to determine current position and completed work"
      pattern: "STATE.md"
---

<objective>
Rewrite the inspect skill from a stub into a fully functional on-demand verification command with scope awareness, Tier 1 structural checks, Tier 2 behavioral checklists, auto-fix support, and celebration feedback.

Purpose: Users need a way to check their work on demand, not just at automatic post-task boundaries. The inspect command is the user-initiated entry point into the verification system. It differs from the build pipeline's automatic verification in two ways: (1) it always shows results even when everything passes (because the user asked), and (2) it always includes Tier 2 behavioral checklist (not just at boundaries). It reuses the same verifier and debugger agents.

Output: Rewritten `skills/inspect/SKILL.md`
</objective>

<execution_context>
@/Users/noahrasheta/.claude/get-shit-done/workflows/execute-plan.md
@/Users/noahrasheta/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-verification/05-CONTEXT.md
@.planning/phases/05-verification/05-RESEARCH.md
@.planning/phases/05-verification/05-01-SUMMARY.md
@skills/inspect/SKILL.md
@skills/build/SKILL.md
@agents/director-verifier.md
@agents/director-debugger.md
@reference/verification-patterns.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite inspect skill with scope-aware verification, Tier 1, Tier 2, auto-fix, and celebration</name>
  <files>skills/inspect/SKILL.md</files>
  <action>
Replace the entire content of skills/inspect/SKILL.md with a complete 7-step verification flow. Keep the same YAML frontmatter (name: inspect, description, disable-model-invocation: true).

**Intro paragraph:**
"You are Director's inspect command. Your job is to verify that what was built actually achieves the user's goals -- not just that tasks were completed, but that the code is real, connected, and working. Unlike the build pipeline's automatic verification (which is invisible on success), the user explicitly asked for this check, so ALWAYS show results."

"Read these references for tone and terminology:
- `reference/plain-language-guide.md`
- `reference/terminology.md`"

**Step 1: Init check**
Check if `.director/` exists. If not, run init script silently. Continue.

**Step 2: Check for completed work**
Read `.director/STATE.md`. Check for any completed tasks.

If nothing built yet:
> "Nothing to check yet. Once you start building, I can verify that everything works the way you intended. Want to get started with `/director:build`?"
Stop here.

**Step 3: Determine scope**
Parse `$ARGUMENTS` to determine verification scope:

- No arguments or empty: scope = current step (smart default per locked decision)
- `"goal"`: scope = current goal (all steps in the goal)
- `"all"`: scope = entire project (all goals)
- Any other text: scope = focused check on that topic (search for matching tasks/steps)

Tell the user what's being checked:
> "Checking [scope description]..."

For example: "Checking the current step...", "Checking your entire project...", "Looking at everything related to login..."

**Step 4: Assemble verification context**
Based on scope, read the relevant files and assemble context for the verifier:

**Step scope (default):**
- Determine current step from GAMEPLAN.md "Current Focus" and STATE.md
- Read the step's STEP.md
- Read all task files (both .md and .done.md) in the step's tasks/ directory
- Read VISION.md

**Goal scope:**
- Determine current goal from GAMEPLAN.md and STATE.md
- Read GOAL.md from the goal directory (if it exists)
- Read ALL step STEP.md files in the goal
- Read ALL task files across all steps in the goal
- Read VISION.md

**All scope:**
- Read GAMEPLAN.md
- Read ALL goal/step/task files in the entire .director/goals/ hierarchy
- Read VISION.md
- For large projects (many goals/steps), prioritize recent and in-progress work. Spawn multiple verifier invocations if needed (one per goal) and aggregate results.

**Focused scope (text argument):**
- Search GAMEPLAN.md, step, and task files for content matching the argument text
- Include matching steps/tasks plus their parent context (STEP.md, GOAL.md)
- Read VISION.md
- If nothing matches: "I couldn't find anything matching '[argument]'. Try `/director:inspect` to check the current step, or `/director:inspect all` for everything."

**Step 5: Run Tier 1 structural verification**
Spawn `director-verifier` via Task tool with the assembled context:

```xml
<task>[Describe what was built based on the scope -- summarize completed tasks]</task>
<instructions>Check all files created or modified by the completed tasks in scope. Look for stubs, orphans, and wiring issues. For each "Needs attention" issue, classify as auto-fixable or not. Report everything -- the user explicitly asked for this check.</instructions>
```

**IMPORTANT:** Unlike the build pipeline, ALWAYS show results here since the user asked:

**If everything is clean:**
> "Everything checks out. The code is properly built and connected -- no stubs, no orphaned files, and everything is wired together."

**If issues found:**
Present in two-severity format (same as build skill):

**"Needs attention"** section with what + why + where for each issue.
**"Worth checking"** section for informational items.

If any auto-fixable issues exist:
> "I can fix [N] of these automatically. Want me to try?"

Wait for user response.

If user approves: run the same auto-fix retry loop as the build skill:
1. Spawn `director-debugger` with issue context
2. Show progress: "Investigating... Found the cause... Applying fix (attempt N of max)..."
3. Check debugger Status line: Fixed / Needs more work / Needs manual attention
4. If Fixed: spawn verifier to re-check, commit fix if passes
5. If not fixed: retry up to cap, then report what was tried
6. Retry caps same as build skill (2 for simple wiring, 3 for stubs, 3-5 for complex)

If user declines: note issues and continue to Step 6.

**Step 6: Run Tier 2 behavioral verification**
Generate a behavioral checklist for the scope being checked. This always runs for inspect (unlike build, which only triggers at boundaries).

Assemble checklist context based on scope:
- Step scope: STEP.md + .done.md task files + VISION.md + relevant git log
- Goal scope: GOAL.md + all STEP.md files + .done.md tasks + VISION.md
- All scope: GAMEPLAN.md + summaries of all goals and their steps + VISION.md

Generate checklist items that are things the user can try and observe. Size based on scope complexity:
- Step scope: 3-7 items
- Goal scope: 5-10 items (focus on cross-step integration)
- All scope: 7-12 items (focus on cross-goal integration)

Present the checklist:
> "Here's a checklist to verify things work as expected:
>
> 1. [Testable action with expected result]
> 2. [Testable action with expected result]
> ...
>
> Try these out and let me know how they go!"

Wait for user response. Interpret their natural-language answers.

**If ALL items pass:** Continue to Step 7 with full pass.
**If SOME items fail (lead with wins per locked decision):**
> "[N] of [M] checks passed! [Failed items] need attention:
> - [Issue description]"
Offer auto-fix if applicable. Otherwise describe what to do.

The checklist is guidance, not a gate. If the user wants to stop without completing it, let them.

**Step 7: Celebrate and report progress**
After checklist results (or after Tier 1 if user skipped checklist):

**If everything passed (both tiers):**
Combine outcome + progress (locked decision):

For step scope:
> "[Outcome: what's working]. [Step name] is verified -- you're [X] of [Y] steps through [goal name]."

For goal scope:
> "[Goal name] is looking solid! Everything that was built is working together. [X] of [Y] goals complete."

For all scope:
> "Your project is in great shape! [Summary of what's verified]. [X] of [Y] goals complete overall."

**If issues remain:**
Summarize what passed, what needs attention, and what the user can do next.

**Language Reminders section:**
Same as build skill -- use Director vocabulary, never mention git, be conversational, celebrate naturally, never blame user.

End with `$ARGUMENTS` on its own line.
  </action>
  <verify>
Read skills/inspect/SKILL.md and confirm:
1. Frontmatter has name: inspect, description, disable-model-invocation: true
2. Step 1: init check exists
3. Step 2: checks for completed work, routing to build if nothing exists
4. Step 3: scope resolution handles no args (current step), "goal", "all", and free text
5. Step 4: context assembly for each scope level (step, goal, all, focused)
6. Step 5: spawns director-verifier, ALWAYS shows results, offers auto-fix with user consent
7. Step 6: Tier 2 behavioral checklist with scope-based sizing, wait for user response
8. Step 7: celebration with progress (combines outcome + progress per locked decision)
9. Auto-fix retry loop matches build skill pattern (debugger spawn, status check, retry cap)
10. Two-severity format used for issues (Needs attention / Worth checking)
11. Partial passes lead with wins (locked decision)
12. $ARGUMENTS on its own line at the end
13. Language reminders present
  </verify>
  <done>Inspect skill is a fully functional on-demand verification command with scope awareness, Tier 1 structural checks, Tier 2 behavioral checklists, consent-based auto-fix, and celebration feedback. Replaces the Phase 1 stub entirely.</done>
</task>

</tasks>

<verification>
The inspect skill handles all user scenarios:
1. /director:inspect (no args) -- checks current step with both tiers
2. /director:inspect goal -- checks current goal with both tiers
3. /director:inspect all -- checks entire project
4. /director:inspect "login" -- focused check on matching content
5. Clean results shown (unlike build, where they're invisible)
6. Issues presented with two-severity format and auto-fix offer
7. Tier 2 checklist always generated (unlike build, where it's boundary-only)
8. Progress celebration after successful verification
</verification>

<success_criteria>
- /director:inspect replaces the stub with a 7-step verification flow (VRFY-10)
- Smart default checks current step; accepts goal/all arguments (locked decision)
- Always shows results even when clean (user explicitly asked)
- Two-severity format for issues (VRFY-07)
- Auto-fix with user consent and debugger retry loop (VRFY-08, VRFY-09)
- Tier 2 behavioral checklist for any scope (VRFY-05, VRFY-06)
- Celebrates completion with progress (VRFY-11, locked decision)
- Partial passes lead with wins (locked decision)
</success_criteria>

<output>
After completion, create `.planning/phases/05-verification/05-03-SUMMARY.md`
</output>
